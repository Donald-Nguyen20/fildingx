from PySide6.QtWidgets import (
    QDialog, QVBoxLayout, QLabel,
    QTextBrowser, QLineEdit, QPushButton, QHBoxLayout,
    QFileDialog, QMessageBox, QComboBox, QFormLayout
)
from PySide6.QtCore import Qt
import os

# c·∫ßn 2 file n√†y n·∫±m c√πng th∆∞ m·ª•c:
# - vector_retriever.py
# - llm_client.py
from vector_retriever import VectorRetriever
from llm_client import create_llm_client, PROVIDERS
from llm_config import load_llm_config, save_llm_config
from hud_widgets import HudPanel


class LLMSettingsDialog(QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("LLM Settings")
        self.setModal(True)
        self.setMinimumWidth(520)

        cfg = load_llm_config()

        layout = QVBoxLayout(self)
        form = QFormLayout()

        self.ed_openrouter = QLineEdit(cfg.get("openrouter_api_key",""))
        self.ed_openrouter.setEchoMode(QLineEdit.Password)
        form.addRow("OpenRouter API key:", self.ed_openrouter)

        self.ed_groq = QLineEdit(cfg.get("groq_api_key",""))
        self.ed_groq.setEchoMode(QLineEdit.Password)
        form.addRow("Groq API key:", self.ed_groq)

        self.ed_gemini = QLineEdit(cfg.get("gemini_api_key",""))
        self.ed_gemini.setEchoMode(QLineEdit.Password)
        form.addRow("Gemini API key:", self.ed_gemini)

        self.ed_ollama_host = QLineEdit(cfg.get("ollama_host","http://localhost:11434"))
        form.addRow("Ollama host:", self.ed_ollama_host)

        layout.addLayout(form)

        btns = QHBoxLayout()
        btns.addStretch(1)
        btn_save = QPushButton("Save")
        btn_cancel = QPushButton("Cancel")
        btn_save.clicked.connect(self.on_save)
        btn_cancel.clicked.connect(self.reject)
        btns.addWidget(btn_save)
        btns.addWidget(btn_cancel)
        layout.addLayout(btns)
        self.setStyleSheet(self.styleSheet() + """
QComboBox {
    background-color: white;
    color: black;
    border: 1px solid #B0B0B0;
    border-radius: 6px;
    padding: 4px 10px;
}
QComboBox QAbstractItemView {
    background-color: white;
    color: black;
    selection-background-color: #DDEEFF;
    selection-color: black;
}
""")

    def on_save(self):
        cfg = load_llm_config()
        cfg["openrouter_api_key"] = self.ed_openrouter.text().strip()
        cfg["groq_api_key"] = self.ed_groq.text().strip()
        cfg["gemini_api_key"] = self.ed_gemini.text().strip()
        cfg["ollama_host"] = self.ed_ollama_host.text().strip() or "http://localhost:11434"
        save_llm_config(cfg)
        self.accept()

class AIChatPopup(QDialog):
    def __init__(self, main_app=None, parent=None):
        super().__init__(parent)
        self.main_app = main_app

        # tr·∫°ng th√°i RAG
        self.store_dir = None
        self.retriever = None
        cfg = load_llm_config()
        self.provider_key = "ollama"
        self.model_name = cfg.get("ollama_model", "llama3.2:3b")
        self.llm = create_llm_client(self.provider_key, self.model_name)



        self.setWindowTitle("AI Chat")
        self.setFixedSize(660, 600)
        self.setWindowFlags(Qt.Dialog | Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint)
        
        self.setAttribute(Qt.WA_TranslucentBackground, True)

        outer = QVBoxLayout(self)
        outer.setContentsMargins(0, 0, 0, 0)

        shell = HudPanel(self, notch=True)
        outer.addWidget(shell)

        layout = QVBoxLayout(shell)
        layout.setContentsMargins(18, 26, 18, 18)  # ch·ª´a ch·ªó notch + padding HUD


        # ===== Top bar: "Chat" + n√∫t Load Vector Store =====
        top_bar = QHBoxLayout()
        top_bar.addWidget(QLabel("üí¨ Chat:"))
        top_bar.addStretch()
        # Provider combobox
        self.cmb_provider = QComboBox()
        for k, label in PROVIDERS:
            self.cmb_provider.addItem(label, userData=k)
        self.cmb_provider.setToolTip("Choose LLM provider")
        top_bar.addWidget(self.cmb_provider)

        # Model combobox (text list)
        self.cmb_model = QComboBox()
        self.cmb_model.setEditable(True)
        self.cmb_model.setToolTip("Choose / type model name")
        top_bar.addWidget(self.cmb_model)

        # Settings button
        self.btn_llm_settings = QPushButton("‚öô")
        self.btn_llm_settings.setToolTip("LLM Settings (API keys / defaults)")
        top_bar.addWidget(self.btn_llm_settings)

        self.btn_load_vs = QPushButton("Load Vector Store")
        self.btn_load_vs.clicked.connect(self.load_vector_store)
        top_bar.addWidget(self.btn_load_vs)

        layout.addLayout(top_bar)

        # ===== Chat display =====
        self.chat_display = QTextBrowser()
        self.chat_display.setOpenExternalLinks(False)
        self.chat_display.setOpenLinks(False)
        layout.addWidget(self.chat_display)
        # ===== INIT provider/model m·∫∑c ƒë·ªãnh (ƒê·∫∂T ·ªû ƒê√ÇY) =====
        self._fill_models_for_provider("ollama")

        # ƒë·∫∑t provider ban ƒë·∫ßu = ollama
        for i in range(self.cmb_provider.count()):
            if self.cmb_provider.itemData(i) == "ollama":
                self.cmb_provider.setCurrentIndex(i)
                break

        # kh·ªüi t·∫°o LLM theo provider/model ƒëang hi·ªÉn th·ªã
        self.on_llm_changed()

        # ===== Input row =====
        self.input_line = QLineEdit()
        self.input_line.setPlaceholderText("Nh·∫≠p tin nh·∫Øn...")
        self.input_line.returnPressed.connect(self.handle_user_input)

        send_btn = QPushButton("G·ª≠i")
        send_btn.clicked.connect(self.handle_user_input)

        input_layout = QHBoxLayout()
        input_layout.addWidget(self.input_line)
        input_layout.addWidget(send_btn)
        layout.addLayout(input_layout)

        # Th√¥ng b√°o tr·∫°ng th√°i
        self.chat_display.append("ü§ñ Tr·ª£ l√Ω: RAG ƒëang ·ªü ch·∫ø ƒë·ªô ch·ªù (ch∆∞a load Vector Store).")

        # (Tu·ª≥ ch·ªçn) n·∫øu main_app ƒë√£ c√≥ last_vector_store_dir th√¨ auto-load
        if self.main_app is not None:
            store = getattr(self.main_app, "last_vector_store_dir", None)
            if store and self._is_valid_store(store):
                self._init_store(store)
                self.chat_display.append(f"‚úÖ ƒê√£ auto-load Vector Store:\n{store}")
        self.btn_llm_settings.clicked.connect(self.open_llm_settings)
        self.cmb_provider.currentIndexChanged.connect(self.on_llm_changed)
        self.cmb_model.currentIndexChanged.connect(self.on_llm_changed)
        combo_qss = """
        QComboBox {
            background-color: #FFFFFF;
            color: #000000;
            border: 1px solid rgba(180, 180, 180, 220);
            border-radius: 8px;
            padding: 4px 10px;
        }
        QComboBox::drop-down {
            border: none;
            width: 18px;
        }
        QComboBox QAbstractItemView {
            background-color: #FFFFFF;
            color: #000000;
            selection-background-color: #DDEEFF;
            selection-color: #000000;
            outline: 0;
        }
        """

        self.cmb_provider.setStyleSheet(combo_qss)
        self.cmb_model.setStyleSheet(combo_qss)

        # √©p Qt ‚Äúv·∫Ω background‚Äù d√π widget cha trong su·ªët (HUD)
        self.cmb_provider.setAttribute(Qt.WA_StyledBackground, True)
        self.cmb_model.setAttribute(Qt.WA_StyledBackground, True)

    def _fill_models_for_provider(self, provider_key: str):
        cfg = load_llm_config()
        self.cmb_model.blockSignals(True)
        self.cmb_model.clear()

        presets = {
            "ollama": [cfg.get("ollama_model","llama3.2:3b"), "llama3.2:3b", "qwen2.5:7b", "deepseek-r1:7b"],
            "openrouter": [cfg.get("openrouter_model","meta-llama/llama-3.3-70b-instruct:free")],
            "groq": [cfg.get("groq_model","llama-3.3-70b-versatile")],
            "gemini": [cfg.get("gemini_model","gemini-1.5-flash"), "gemini-1.5-pro"],
        }
        for m in presets.get(provider_key, []):
            if m:
                self.cmb_model.addItem(m)

        self.cmb_model.setCurrentIndex(0 if self.cmb_model.count() else -1)
        self.cmb_model.blockSignals(False)

    def on_llm_changed(self):
        provider = self.cmb_provider.currentData()
        model = self.cmb_model.currentText().strip()

        # n·∫øu v·ª´a ƒë·ªïi provider, refill models
        if provider != getattr(self, "provider_key", None):
            self.provider_key = provider
            self._fill_models_for_provider(provider)
            model = self.cmb_model.currentText().strip()

        try:
            self.model_name = model
            self.llm = create_llm_client(provider, model)
            self.chat_display.append(f"<i>‚úÖ LLM: {self.cmb_provider.currentText()} | {model}</i>")
        except Exception as e:
            self.chat_display.append(f"<i>‚ö†Ô∏è Cannot init LLM: {e}</i>")
            # fallback
            self.provider_key = "ollama"
            self._fill_models_for_provider("ollama")
            self.llm = create_llm_client("ollama", self.cmb_model.currentText().strip())
            self.chat_display.append("<i>‚Ü© Fallback to Ollama</i>")

    def open_llm_settings(self):
        dlg = LLMSettingsDialog(self)
        if dlg.exec():
            # reload model list + llm after saving settings
            self._fill_models_for_provider(self.cmb_provider.currentData())
            self.on_llm_changed()

    def _is_valid_store(self, folder: str) -> bool:
        required = ["index.faiss", "metadata.json", "base_path.txt"]
        return all(os.path.exists(os.path.join(folder, f)) for f in required)

    def _init_store(self, folder: str):
        self.store_dir = folder
        self.retriever = VectorRetriever(folder)
        # l∆∞u l·∫°i v·ªÅ main_app ƒë·ªÉ app ch√≠nh nh·ªõ store ƒëang d√πng
        if self.main_app is not None:
            self.main_app.last_vector_store_dir = folder

    def load_vector_store(self):
        folder = QFileDialog.getExistingDirectory(self, "Select Vector Store Folder")
        if not folder:
            return

        if not self._is_valid_store(folder):
            QMessageBox.warning(
                self,
                "Invalid Vector Store",
                "B·∫°n ch·ªçn sai folder.\n\nFolder ƒë√∫ng ph·∫£i ch·ª©a:\n"
                "- index.faiss\n- metadata.json\n- base_path.txt"
            )
            return

        self._init_store(folder)
        self.chat_display.append(f"‚úÖ Vector Store loaded:\n{folder}")

    def handle_user_input(self):
        user_input = self.input_line.text().strip()
        if not user_input:
            return

        self.chat_display.append(f"üßë B·∫°n: {user_input}")
        self.input_line.clear()

        # N·∫øu ch∆∞a load store ‚Üí nh·∫Øc user load
        if self.retriever is None:
            self.chat_display.append("ü§ñ Tr·ª£ l√Ω: B·∫°n h√£y b·∫•m 'Load Vector Store' tr∆∞·ªõc nh√©.")
            return

        # ==== RAG Retrieve ====
        results = self.retriever.search(user_input, top_k=12)
        if not results:
            self.chat_display.append("ü§ñ Tr·ª£ l√Ω: M√¨nh kh√¥ng t√¨m th·∫•y ƒëo·∫°n li√™n quan trong Vector Store.")
            return

        # Gh√©p CONTEXT (gi·ªõi h·∫°n ƒë·ªÉ model 3B kh√¥ng ng·ª£p)
        ctx_blocks = []
        for i, r in enumerate(results, start=1):
            ctx_blocks.append(f"[{i}] {r['file_name']} | chunk {r['chunk_id']} | score={r['score']:.3f}\n{r['text']}")
        context = "\n\n".join(ctx_blocks)
        context = context[:8000]

        prompt = (
            "You are a helpful assistant. Use ONLY the context below to answer.\n"
            "If the answer is not in the context, say you don't have enough information.\n\n"
            f"CONTEXT:\n{context}\n\n"
            f"QUESTION:\n{user_input}\n\n"
            "ANSWER:\n"
        )

        try:
            answer = self.llm.generate(prompt)
        except Exception as e:
            answer = f"(LLM error) {e}"

        self.chat_display.append(f"ü§ñ Tr·ª£ l√Ω: {answer}")

        # (Gi·ªØ UI nh∆∞ c≈©) ‚Äî ch·ªâ append ngu·ªìn t√≥m t·∫Øt v√†o chat
        src_lines = []
        for i, r in enumerate(results, start=1):
            src_lines.append(f"[{i}] {r['rel_path']} (chunk {r['chunk_id']})")
        self.chat_display.append("üìå Sources:\n" + "\n".join(src_lines))
        self.chat_display.setStyleSheet("""
        QTextBrowser {
            background: rgba(8, 14, 20, 160);
            border: 1px solid rgba(0, 220, 255, 90);
            border-radius: 10px;
            color: #d9ffff;
            padding: 10px;
        }
        """)

        self.input_line.setStyleSheet("""
        QLineEdit {
            background: rgba(8, 14, 20, 180);
            border: 1px solid rgba(0, 220, 255, 110);
            border-radius: 10px;
            color: #d9ffff;
            padding: 8px 10px;
        }
        QLineEdit:focus {
            border: 1px solid rgba(0, 220, 255, 220);
        }
        """)
